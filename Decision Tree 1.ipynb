{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6908894d-579e-4bdb-8959-1e10e3f2b759",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57fa9d-5634-463a-b58d-075a05a7f8b2",
   "metadata": {},
   "source": [
    "Sure, here is a line-by-line description of the decision tree classifier algorithm:\n",
    "\n",
    "1)A decision tree classifier is a supervised learning algorithm used for both classification and regression tasks.\n",
    "\n",
    "2)It works by recursively splitting the data into subsets based on the feature that provides the highest information gain or the lowest Gini impurity, depending on the criterion used.\n",
    "\n",
    "3)Each internal node of the tree represents a decision based on a feature, and each branch represents the outcome of that decision.\n",
    "\n",
    "4)The process continues until the algorithm reaches a stopping condition, such as a maximum tree depth or a minimum number of samples per leaf.\n",
    "\n",
    "5)The leaf nodes of the tree contain the final predictions.\n",
    "\n",
    "6)To make predictions, the algorithm traverses the tree from the root node to a leaf node by following the path determined by the feature values of the input sample.\n",
    "\n",
    "7)The output is the class label assigned to that leaf node.\n",
    "\n",
    "8)Decision trees are easy to interpret and can handle both numerical and categorical data but are prone to overfitting, which can be mitigated through techniques like pruning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa354383-cbe8-4417-ab43-2afd1ba8a78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7af106c0-8283-4d2f-bdbf-e61e0e7f3143",
   "metadata": {},
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81458ae-06df-4583-919f-5dce689d780a",
   "metadata": {},
   "source": [
    "Mathematical Intuition Behind Decision Tree Classification-\n",
    "\n",
    "Core Concept\n",
    "Decision trees aim to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.   \n",
    "\n",
    "Steps Involved\n",
    "\n",
    "1. Entropy\n",
    "Definition: Measures the impurity or randomness in a dataset.\n",
    "\n",
    "Formula:\n",
    "Entropy(S) = - Σ [p(i) * log2(p(i))]\n",
    "\n",
    "Where:\n",
    "S is the dataset\n",
    "p(i) is the probability of class i in S\n",
    "Interpretation: Higher entropy indicates more impurity, lower entropy indicates purer data.\n",
    "\n",
    "2. Information Gain\n",
    "Definition: Measures the decrease in entropy after a dataset is split on an attribute.\n",
    "\n",
    "Formula:\n",
    "Information Gain(S, A) = Entropy(S) - [Weighted Average of Entropy(Sv)]\n",
    "\n",
    "Where:\n",
    "S is the dataset\n",
    "A is the attribute\n",
    "Sv is the subset of S with attribute A having value v\n",
    "Interpretation: Higher information gain indicates a better split.\n",
    "\n",
    "3. Splitting the Node\n",
    "Objective: Find the attribute with the highest information gain to split the dataset.\n",
    "Process:\n",
    "Calculate information gain for each attribute.\n",
    "Select the attribute with the highest information gain as the splitting attribute.\n",
    "Divide the dataset into subsets based on the values of the selected attribute.\n",
    "\n",
    "4. Creating Child Nodes\n",
    "For each subset created in step 3, a new child node is created.\n",
    "The process of calculating entropy, information gain, and splitting is recursively applied to each child node.\n",
    "\n",
    "5. Stopping Criteria\n",
    "The tree growth is stopped when:\n",
    "All data points in a node belong to the same class.\n",
    "There are no more attributes to split on.\n",
    "The depth of the tree reaches a predefined limit.\n",
    "The number of data points in a node is below a predefined threshold.\n",
    "\n",
    "Key Points-\n",
    "\n",
    "1)Decision trees are essentially a greedy algorithm, making the best decision at each step without considering future consequences.\n",
    "2)The choice of splitting criteria (entropy, information gain, Gini index) can impact the tree structure.\n",
    "3)Pruning can be applied to simplify the tree and improve generalization.\n",
    "4)Decision trees are susceptible to overfitting, especially with noisy data.\n",
    "\n",
    "Additional Considerations\n",
    "1)Handling Missing Values: Imputation or ignoring instances with missing values.\n",
    "2)Continuous Attributes: Discretization or using information gain ratio.\n",
    "3)Overfitting: Pruning, cross-validation, and ensemble methods.\n",
    "\n",
    "By understanding these core concepts and steps, you can grasp the mathematical foundation of decision tree classification.\n",
    "\n",
    "Would you like to delve deeper into a specific aspect of decision trees, such as pruning, handling continuous attributes, or ensemble methods?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500a50b-1af9-4ed1-87aa-6caaa7604584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a83f5ccb-0058-4f05-9ca0-79c1942f4a3e",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f734b-f69d-4f04-b1db-f5a6835c1e08",
   "metadata": {},
   "source": [
    "Here's how a decision tree classifier can be used to solve a binary classification problem:\n",
    "\n",
    "1)Binary Classification Definition: Binary classification involves categorizing data into one of two distinct classes or categories.\n",
    "\n",
    "2)Feature Selection: The algorithm starts by evaluating each feature to determine which one best separates the data into the two classes.\n",
    "\n",
    "3)Choosing a Split: It selects the feature and the threshold that provide the highest information gain or lowest Gini impurity, effectively splitting the dataset into two groups that are more homogeneous in terms of class labels.\n",
    "\n",
    "4)Creating Nodes: The chosen feature becomes a decision node, and the dataset is divided into two branches based on the decision.\n",
    "\n",
    "5)Recursive Splitting: The process of selecting features and creating nodes is recursively applied to each resulting subset until a stopping criterion is met, such as reaching a maximum tree depth or achieving pure leaf nodes.\n",
    "\n",
    "6)Leaf Nodes: Once no further splitting is required or possible, the algorithm assigns a class label to each leaf node, representing one of the two classes.\n",
    "\n",
    "7)Making Predictions: For a new data instance, the algorithm traverses the tree from the root, following the path defined by the feature values of the instance, until it reaches a leaf node.\n",
    "\n",
    "8)Output: The class label of the leaf node is assigned as the prediction for that data instance.\n",
    "\n",
    "9)Handling Overfitting: Techniques like pruning, which removes branches that provide little predictive power, can be applied to improve generalization and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18da61-7573-4505-b453-16bdc8b627d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1bb6f22-2b9e-42c5-a8bf-d50241470b6b",
   "metadata": {},
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d609b-e8d0-461e-82e3-319a27dbc950",
   "metadata": {},
   "source": [
    "Here's the geometric intuition behind decision tree classification and how it can be used to make predictions:\n",
    "\n",
    "1)Data Space Partitioning: A decision tree classifier partitions the feature space into rectangular regions by making axis-aligned splits based on feature values.\n",
    "\n",
    "2)Hierarchical Splitting: Each split in the tree corresponds to a hyperplane that is perpendicular to one of the feature axes, effectively dividing the data space into two regions.\n",
    "\n",
    "3)Recursive Division: The recursive nature of the tree means that each node divides the data space further, creating smaller and more specific regions.\n",
    "\n",
    "4)Leaf Nodes as Regions: Each leaf node in the tree represents a distinct region in the feature space where all points are assigned the same class label.\n",
    "\n",
    "5)Geometric Interpretation: The decision boundaries are a series of straight lines (in 2D) or hyperplanes (in higher dimensions) that separate the classes in the feature space.\n",
    "\n",
    "6)Decision Path: For making predictions, a new data instance follows a path from the root node to a leaf node based on its feature values, effectively navigating through the partitioned space.\n",
    "\n",
    "7)Class Assignment: The instance is assigned the class label of the region (leaf node) it falls into, based on the majority class of the training instances in that region.\n",
    "\n",
    "8)Flexibility and Interpretability: The geometric simplicity of decision trees allows for easy visualization and interpretation, as well as handling both linear and non-linear decision boundaries.\n",
    "\n",
    "9)Limitations: While decision trees can model complex decision boundaries, they may also create overly complex models that overfit the data, especially with high-dimensional data.\n",
    "\n",
    "In summary, the geometric intuition of decision trees involves partitioning the feature space into axis-aligned regions, allowing for intuitive and interpretable decision-making based on feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cc78c-35aa-4f55-ae39-a50edeb81e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "104a9bc5-e135-4e1d-80c4-da350d507518",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2f5de-4660-4cf1-ba6b-0bce7dd3348b",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "A confusion matrix is a performance evaluation tool used in classification problems. It's a table that summarizes the performance of a classification algorithm by comparing the predicted class labels with the actual class labels.\n",
    "\n",
    "Components\n",
    "-True Positive (TP): Correctly predicted positive cases.\n",
    "-True Negative (TN): Correctly predicted negative cases.\n",
    "-False Positive (FP): Incorrectly predicted as positive (Type I error).\n",
    "-False Negative (FN): Incorrectly predicted as negative (Type II error).\n",
    "\n",
    "Evaluating Model Performance\n",
    "\n",
    "The confusion matrix provides insights into various performance metrics:\n",
    "\n",
    "1)Accuracy: Overall correctness of the model.\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "2)Precision: Proportion of positive predictions that were actually correct.\n",
    "Precision = TP / (TP + FP)\n",
    "3)Recall (Sensitivity): Proportion of actual positive cases that were correctly identified.\n",
    "Recall = TP / (TP + FN)\n",
    "4)Specificity: Proportion of actual negative cases that were correctly identified.\n",
    "Specificity = TN / (TN + FP)\n",
    "5)F1-score: Harmonic mean of precision and recall.\n",
    "F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "By analyzing these metrics, you can understand the strengths and weaknesses of your classification model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f731e-ff6f-4b9c-ab39-2a3ed1b97263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267fc6c2-4bf3-4c52-ab38-aee1105f5a60",
   "metadata": {},
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606e078-76a4-4aad-aa93-7e59a3772c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8888888888888888\n",
      "Recall: 0.8\n",
      "F1-score: 0.8421052631578948\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a confusion matrix as a NumPy array\n",
    "confusion_matrix = np.array([[80, 20],\n",
    "                             [10, 90]])\n",
    "\n",
    "# Calculate precision\n",
    "precision = confusion_matrix[0, 0] / (confusion_matrix[0, 0] + confusion_matrix[1, 0])\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate recall\n",
    "recall = confusion_matrix[0, 0] / (confusion_matrix[0, 0] + confusion_matrix[0, 1])\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1-score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964195a9-7024-4e86-8dce-87866e935d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63be37ba-2208-40b7-96c3-bed93f6f1b62",
   "metadata": {},
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cedda-2821-47aa-9855-1a3adc517869",
   "metadata": {},
   "source": [
    "Importance of Choosing an Appropriate Evaluation Metric\n",
    "Selecting the right evaluation metric is crucial for assessing a classification model's performance accurately. A metric that is suitable for one problem might not be ideal for another. For instance, in a medical diagnosis scenario, prioritizing recall (sensitivity) is essential to minimize false negatives, even if it comes at the cost of lower precision.\n",
    "\n",
    "Key Metrics and Their Use Cases\n",
    "1)Accuracy: Overall correctness, suitable for balanced datasets.\n",
    "2)Precision: Proportion of positive predictions that were correct, ideal when false positives are costly.\n",
    "3)Recall (Sensitivity): Proportion of actual positive cases correctly identified, crucial when false negatives are critical.\n",
    "4)F1-score: Harmonic mean of precision and recall, provides a balance between the two.\n",
    "5)Confusion Matrix: Provides a detailed breakdown of model performance, essential for understanding error patterns.\n",
    "\n",
    "Code Example for Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d0fef1-dcbb-4e7f-876c-cda4a3c2da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(confusion_matrix):\n",
    "  \"\"\"Calculates precision, recall, and F1-score from a confusion matrix.\n",
    "\n",
    "  Args:\n",
    "    confusion_matrix: A NumPy array representing the confusion matrix.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of precision, recall, and F1-score.\n",
    "  \"\"\"\n",
    "\n",
    "  tp = confusion_matrix[0, 0]\n",
    "  fp = confusion_matrix[1, 0]\n",
    "  fn = confusion_matrix[0, 1]\n",
    "  tn = confusion_matrix[1, 1]\n",
    "\n",
    "  precision = tp / (tp + fp)\n",
    "  recall = tp / (tp + fn)\n",
    "  f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "  return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a966553-6e10-405e-88b2-5dba49a077eb",
   "metadata": {},
   "source": [
    "Choosing the Right Metric\n",
    "\n",
    "To select the appropriate metric, consider the following factors:\n",
    "\n",
    "1)Class imbalance: If the dataset is imbalanced, accuracy might be misleading. Precision, recall, and F1-score can provide more informative insights.\n",
    "2)Cost of errors: If false positives are more costly, prioritize precision. If false negatives are more critical, focus on recall.\n",
    "3)Business objectives: Align the metric with the specific goals of the project. For example, in fraud detection, low false positives are crucial.\n",
    "By carefully considering these factors and utilizing the appropriate evaluation metrics, you can effectively assess your classification model's performance and make informed decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53578c-704b-4294-a6ca-2d3ffbe8fba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61be6eb8-cb35-4236-85e8-1a970d84b0c1",
   "metadata": {},
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adaf9f3-537a-4d99-b7af-cfb9bef0133e",
   "metadata": {},
   "source": [
    "Precision-Critical Classification: Fraud Detection\n",
    "\n",
    "Problem: Identifying fraudulent transactions in a large dataset of financial transactions.\n",
    "\n",
    "Why Precision is Crucial:\n",
    "\n",
    "1)False positives (FP): Incorrectly flagging a legitimate transaction as fraudulent. This can lead to inconvenience for customers, damage to customer relationships, and unnecessary investigations.\n",
    "\n",
    "2)False negatives (FN): Incorrectly classifying a fraudulent transaction as legitimate. This is obviously catastrophic, leading to financial loss.\n",
    "\n",
    "While minimizing false negatives is important, it's often more critical to minimize false positives in fraud detection. A model with high precision ensures that when a transaction is flagged as fraudulent, there's a high probability that it's actually fraudulent, reducing the number of false alarms and unnecessary investigations.\n",
    "\n",
    "In essence, precision is prioritized because the cost of a false positive (inconvenience and potential loss of customers) is often higher than the cost of a false negative (further investigation required).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f434ca3-d733-4089-ac03-758b374de88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fdaa0b0-008a-45df-80e0-e836c99e5e74",
   "metadata": {},
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be106a0-9e19-4183-86e3-da47b5731b15",
   "metadata": {},
   "source": [
    "Recall-Critical Classification: Cancer Detection\n",
    "\n",
    "Problem: Identifying patients with cancer from medical imaging data.\n",
    "\n",
    "Why Recall is Crucial:\n",
    "\n",
    "1)False negatives (FN): Incorrectly classifying a patient with cancer as healthy. This is a catastrophic error as it can lead to delayed treatment and potentially fatal consequences.\n",
    "\n",
    "2)False positives (FP): Incorrectly classifying a healthy patient as having cancer. While this can lead to unnecessary tests and anxiety, it's generally less harmful than a false negative.\n",
    "\n",
    "In cancer detection, the priority is to identify as many cancer cases as possible, even if it means some healthy patients might undergo further tests. A high recall ensures that most patients with cancer are correctly identified, allowing for timely intervention and treatment.\n",
    "\n",
    "In essence, recall is prioritized because the cost of a false negative (delayed or missed treatment) is significantly higher than the cost of a false positive (further testing).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f020ebd-529b-420f-8bb6-ddd9fd2835a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b8b9d-e62e-4511-8b87-cff4de8e4fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974cfcd-c050-4880-9b3b-dc730990c48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8850da4-373b-4499-a2b0-172b35c1cdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d06f2-6826-4108-b876-d1d02b607951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93997752-f233-402f-b576-697658cac8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beacbab-36ab-4312-81c2-42f450f4caa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ddef6-213a-4e80-8d88-6328ba8c1775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e8f01-41f0-47c9-9368-a6128902652b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c515a2f-be3c-492e-8efd-668e522e3d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637dda7-eb56-4aad-9004-a316d29e3975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8c8c1-b468-4efc-9d8e-bfb06b820c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfc054-3a12-4124-88e4-6c84f97e2ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2141277-5a8d-44d3-833f-ccd6f2846dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a971ab3-ded6-4f62-a502-febb2cfec859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706721c5-733f-4b5b-b8c2-74e55980317c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c6ded-9767-4469-971f-7bef9940ee50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81785be3-224a-4ee3-aa72-7e4817fdb0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90cbac-dce1-49b9-af06-45e9f140efbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e333552-a63d-47a7-9f7f-c44de686fe17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44afcf2e-0a34-48ee-b666-aa3a5d662e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a464c2ef-7f5f-4eba-ae3d-0622f29328b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
